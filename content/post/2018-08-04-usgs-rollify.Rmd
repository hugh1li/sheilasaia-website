---
title: "Using tibbletime::rollify with USGS Streamgage Data"
author: "Sheila Saia"
date: 2018-08-09T21:12:00-05:00
slug: usgs-rollify
categories: ["R"]
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

# Background

While attending rstudio::conf 2018, I heard about the [tibbletime package](https://cran.r-project.org/web/packages/tibbletime/tibbletime.pdf) developed by Davis Vaughan and Matt Dancho for analysis of time series data. In his [conference talk](https://www.rstudio.com/resources/videos/the-future-of-time-series-and-financial-analysis-in-the-tidyverse/), Davis Vaughan presented several business/finance examples to showcase `tibbletime`'s functionality and mentioned a few, general non-business applications at the end of his talk. I couldn't help but think about how this package might be especially helpful for environmental scientists working with time series data. I really liked `rollify()`, which could be used to create custom moving window functions (e.g., `mean()`) to time series data; where the size of the window (e.g., 3 days) could also be specified. I was aware of `movingAve()` and `movingDiff()` functions in the [smwrBase package](https://pubs.usgs.gov/of/2015/1202/downloads/appendix1.pdf) developed by the U.S. Geological Survey (USGS), which calculate moving averages and differences, respectively. However, `rollify()` seemed like it had a more flexibility; it could take a rolling standard deviation or a rolling ratio of two functions. The possibilities were endless.

Shortly after the conference, I also heard about the [dataRetreival package](https://cran.r-project.org/web/packages/dataRetrieval/dataRetrieval.pdf) developed by the USGS. This package allows users to import USGS streamflow data (and any other data associated with [USGS streamgages](https://water.usgs.gov/nsip/definition9.html)) directly into R.

# Goals of this Post

This all sounded so amazing and I had to try it out, which brings me to the goals of this blog post:

- Use the `dataRetreival` package to download streamflow data for two USGS gages into R
- Use the `tibbletime` package (along with other `tidyverse` packages) to aggregate these data for different time periods

A special thanks to [Dr. Kristina Hopkins](https://twitter.com/kghopkin?lang=en) at the USGS in Raleigh, NC for sharing her `dataRetrieval` code!

First let's load the libraries we'll need to run the code in this post.

```{r load libraries, message = FALSE}
library(tidyverse)
library(lubridate)
library(tibbletime)
library(dataRetrieval)

```

Before downloading the data you'll have to specify the date range you want and also look up the USGS gage identification numbers for the streamgages of interest. To find these id numbers you can go to the USGS [Water of the Nation website](https://waterdata.usgs.gov/nwis/inventory) and looked up the numbers for sites near Durham, NC. Here, let's choose two: Eno River at Hillsborough, NC (#02085000) and Eno River at Durham, NC (#02085070). The Hillsborough site is upstream of Durham and is less developed.

```{r define sites and dates}
# define start and end dates
start_date <- "2017-01-01"
end_date <- "2018-01-01"

# define sites (you can also just use one site here)
site_numbers <- c("02085070", "02085000")

# save site info to a variable so we can look at it later
site_info <- readNWISsite(site_numbers)

# look at site info
site_info$station_nm

```
 
 Now we need to figure out what parameter code is associated with streamflow. To figure this out, we can save  `parameterCdFile` to a variable and `View()` to see the parameter code look-up table.
 
```{r parameter lookup}
# look for parameters you want
parameter_code_metadata <- parameterCdFile

# we want discharage in cubic feet per second which is code "00060"
my_parameter_code <- "00060"

```
 
 After doing this we see that 00060 is the code for discharge in cubic feet per second (cfs). Now we're ready to download streamflow data!
 
```{r download data}
discharge_raw <- readNWISuv(site_numbers, 
                            my_parameter_code, 
                            tz='America/Jamaica', 
                            start_date, end_date)

```
 
 Let's tidy the data up a little bit. We'll change some of the column names so they make more sense later, add a column with the site name, and select only the most important columns. If you want to do some QA/QC be sure to check out the discharge_code column to make sure you're not missing data or there wasn't something wrong with the gage.
 
```{r tidy data}
discharge = discharge_raw %>%
  select(site_number = site_no, 
         date_time = dateTime,
         discharge_cfs = X_00060_00000,
         discharge_code = X_00060_00000_cd,
         time_zone = tz_cd) %>%
  mutate(site_name = case_when(
    site_number == "02085000" ~ "eno_rv_hillsb",
    site_number == "02085070" ~ "eno_rv_durham")) %>%
  select(site_name, date_time, discharge_cfs)

```
 
Using `head()` we see that we've downloaded 15-min intervals of streamflow data for both sites. Let's say we need daily discharge data to calibrate a hydrology model. We can aggregate the 15-min data to daily data pretty easily using `tidyverse` functions.

```{r daily data via tidyverse}
# look at discharge
head(discharge, n = 10)

# calculate daily average discharage
discharge_daily <- discharge %>%
  mutate(year = year(date_time), 
         month = month(date_time),
         day = day(date_time)) %>%
  mutate(date = ymd(paste0(year, "-", month, "-", day))) %>%
  select(site_name, date, discharge_cfs) %>%
  group_by(site_name, date) %>%
  summarize(avg_daily_discharge_cfs = mean(discharge_cfs))

```

 Now let's try plotting the hydrograph (i.e., average daily discharge vs time) for both sites!
 
```{r plotting daily data}
ggplot(data = discharge_daily) +
  geom_line(aes(x = date, y = avg_daily_discharge_cfs, color = site_name)) +
  facet_wrap(~site_name, nrow = 2, ncol = 1) +
  ylab("Average Daily Discharge (cfs)") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

```
 
As expected (remember the Eno River at Hillsborough site is upstream of the Eno River at Durham site), we see that the two sites mimic one another over this one year time period. We also see that the Eno River at Durham site (top, red line) has some higher peaks than the Eno River at Hillborough site (bottom, teal line). This also makes sense since the Durham site is surrounded by more urbanized land than the Hillsborough site.

Ok, so let's get back to that `tibbletime` package. What if we want to look at different time spans? That is, not just daily, but maybe hourly or half a day. First we have to define our rolling functions and time spans.

For this post, let's create half hour, hourly, half day, and day rolls.
 
```{r defining rolling functions}
# create rollify functions
mean_roll_30min <- rollify(mean, window = 2)
mean_roll_1hr <- rollify(mean, window = 4)
mean_roll_12hr <- rollify(mean, window = 48)
mean_roll_1d <- rollify(mean, window = 96)

```
 
 In this post we'll focus on using the mean in our rolling functions but I can think of several other functions that might be useful for streamgage data lovers: difference, standard deviation, or maybe even the ratio between two calculations. You could even write your own function and roll that. Pretty flexible, right?
 
 After you you write the function `my_roll_function <- rollify(<function>, window = <row span>)` then you can call `my_roll_function(<original data column>)` in `mutate()` to create a new column. Depending on the function you're using, you could maybe even do a nested roll based on a previous column you created. This seems helpful if you're taking differences or summations across different time scales.
 
 Let's roll (our data)! 
 
```{r rolling data}
discharge_roll <- discharge %>%
  group_by(site_name) %>%
  mutate(
    halfhr_avg_discharge_cfs = mean_roll_30min(discharge_cfs),
    hourly_avg_discharge_cfs = mean_roll_1hr(discharge_cfs),
    twelvehr_avg_discharge_cfs = mean_roll_12hr(discharge_cfs), 
    daily_avg_discharge_cfs = mean_roll_1d(discharge_cfs)) %>%
  na.omit()

# use na.omit() to delete some of the first cells in the dataframe that are NA's due to rolling

```
 
 Let's look at the data.
 
```{r looking at rolled data}
head(discharge_roll, n = 10) 

# or use View(discharge_roll) in your RStudio console.

```
 
The data frame has maintained it's length (except for the NA columns at the top we deleted using `na.omit()`) and there are four new columns with aggregations of discharge at the time scales we specified in our rolling functions.
 
 We can plot this as a hydrograph as we did above or we can also plot each aggregation period against the original 15-min data to see how they compare. Let's show the latter.
 
```{r plot aggregated time scales}
my_colors <- c("a_half_hour" = "grey20", "b_hour" = "grey40", "c_half_day" = "grey60", "d_day" = "grey80")

ggplot(data = discharge_roll) +
  geom_point(aes(x = discharge_cfs,y = halfhr_avg_discharge_cfs, color = "a_half_hour"), size = 1) +
  geom_point(aes(x = discharge_cfs, y = hourly_avg_discharge_cfs, color = "b_hour"), size = 1) +
  geom_point(aes(x = discharge_cfs, y = twelvehr_avg_discharge_cfs, color = "c_half_day"), size = 1) +
  geom_point(aes(x = discharge_cfs, y = daily_avg_discharge_cfs, color = "d_day"), size = 1) +
  facet_wrap(~ site_name, ncol = 1, nrow = 2) +
  scale_color_manual(name = "Time Scale", values = my_colors) +
  xlab("15-min Dicharge (cfs)") +
  ylab("Aggregated Discharge (cfs)") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"))

```
 
 There are some interesting patterns that (I think) are worth pointing out:
 
 - Based on the daily hydrograph we plotted above, the Eno River at Durham (top) data set has higher peak events. This was expected given the daily hydrograph we plotted above. We see that discharge values go up to over 10,000 cfs at the Eno River at Durham site whereas for the Eno River at Hillsborough they only go up to about 5,000 cfs.
 
 - I thought it was interesting that the half hour and hourly data sets match the 15-min time scale closely; we see the points follow the 1:1 line for both. However, the half day and daily data show dampened responses with lower peaks and skew away from the 1:1 line. This dampening wasn't completely surprising to me but it made me wonder how this would impact downstream applications of daily streamflow data. Is this dampening represented by a linear or non-linear relationship? Is this relationship a function of watershed properties or near gage properties. Lot's of questions like this come to mind. Most of which deal with being able to account for and maybe even predict this dampening.
 
 - The last thing that I wanted to point out was the spiraling nature of these figures. The more we aggregate the data, the more it seems to spiral into the origin at zero. Also, it's interesting to compare the overlap of time scale spirals (i.e., half day and daily) between the two gages. More specifically, we see that one ring of the half day and daily spirals overlap for the Eno River at Hillsborough site but we don't see this overlap at the Eno River at Durham site. I wonder why? Is it because urban development has caused a little more disconnection between water stored in the landscape for the Eno River at Durham site? Do antecedent moisture conditions play a larger role in future stream response in the less developed Eno River at Hillsborough site?
 
There are still so many questions floating around in my head but figuring out how to integrate the `tibbletime` and `dataRetreival` packages has been fun! If you know of any publications that address these questions or just want to say `Hi!`, please let me know [via Twitter](https://twitter.com/sheilasaia?lang=en).
 